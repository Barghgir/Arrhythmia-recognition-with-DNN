# -*- coding: utf-8 -*-
"""Barghgir_ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1geq82F_tHrrLZ-sy4SRBU0ICXE31hp6u
"""

!pip install wfdb

### libraries

import pandas as pd
import numpy as np
import wfdb
import urllib.request
import matplotlib.pyplot as plt

# !pip install keras
import csv
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Embedding, MaxPooling1D
from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from sklearn.preprocessing import LabelEncoder
import time
from keras import metrics

print('import done')
### List of records
records = wfdb.io.get_record_list(db_dir = 'mitdb', records='all')
print(records )

### Dataframe of voltages

list_volt_0 = []
for i in records:
  image = wfdb.rdrecord(i, pn_dir='mitdb', sampto = 648000, channels=[0])
  signals_array, fields_dictionary = wfdb.rdsamp(i, pn_dir='mitdb', sampto = 648000, channels=[0])
  voltages = pd.DataFrame(signals_array)
  list_volt_0.append(voltages)

df_volt_0 = pd.concat(list_volt_0, axis=1) 
df_volt_0.columns = records 
df_volt_0.index.name = 'Time'
df_volt_0 = df_volt_0.reset_index()
df_volt_0 = pd.melt(df_volt_0,
                       id_vars = 'Time',
                       var_name="Patient",
                       value_name="Voltage_L2")
df_volt_0

# adjusting df for merging at a later time
df_volt = df_volt_0
df_volt['Window'] = [i for i in range(8640) for _ in range(3600)] 
#8640 = 31,104,444(48 patients * sampleto number) / 3600

time_adj = pd.concat([pd.Series(range(3600))] * 8640, axis=0)
time_adj = pd.DataFrame(time_adj.reset_index(drop=True))
time_adj.columns = ['Time_Adj']
df_list = [df_volt, time_adj]
df_volt = pd.concat(df_list, axis=1)
df_volt

### Annotations dataframe

list_ann = []
for i in records:
    ann = wfdb.rdann(i, 'atr', sampto = 648000, pn_dir='mitdb')
    labels_ann = ann.symbol
    annotated_time = ann.sample
    ann_df = pd.DataFrame({'Patient': i, 'Time':annotated_time, 'Annotation': labels_ann})
    list_ann.append(ann_df)
    
anns = pd.concat(list_ann, axis=0)
anns = anns.set_index(['Patient', 'Time'])['Annotation'].reset_index()
anns = anns.replace({'+':'N'})

anns['Results'] = np.where(anns['Annotation'] == 'x', 1, 0) #U is noisey data, labelled as 1
#anns

total = anns['Results'].sum()
print(total)

### Combining Windows Column from Voltage DF to Annotations DF

anns = df_volt.merge(anns, how='left', on = ['Patient', 'Time'])
anns = anns.pivot_table(values='Results', index='Window', aggfunc=np.sum)
anns.Results = np.where(anns['Results'] == 0, 0, 1)
#anns

### Merge Voltages and Annotations
mitdb_df = df_volt.merge(anns, how='left', on = ['Window'])
mitdb_df

#### Dataframe by ECG Window
# Ignored Patient
# Results in 0 (normal ECG window) or 1 (abnormal ECG window)

df_by_window = mitdb_df.set_index(['Window','Patient','Results','Time_Adj'])['Voltage_L2'].unstack().reset_index()

#df_by_window
# df_by_window.to_csv('ECG_window_df.csv')

df_by_window.to_csv('ECG_window_df.csv')

df_by_window

"""Now we use them"""

#---Dataset: Previously sectioned ECG recordings into 2-second windows---

df = pd.read_csv('/content/ECG_window_df.csv')

print('Dataset before the split:')
print(df.shape)

#---Train and Test split manually (test with patient 233 and 234 ECG windows)---

train = df.iloc[0:36900] 
test = df.iloc[-6300:]

X_train = train[train.columns[3:723]] #voltages, train
X_test = test[test.columns[3:723]]    #voltages, test
Y_train = train.Results               #results, train
Y_test = test.Results                 #results, test


print('Train Shape - voltages, results:')
print(X_train.shape, Y_train.shape)
print('Test Shape - voltages, results:')
print(X_test.shape, Y_test.shape)

#---Model---

batch = 16
epochs = 10
shape = np.size(X_train,1)


model = Sequential()
model.add(Dense(100, activation='relu', input_shape = (shape,1)))
model.add(Dropout(0.1))
model.add(Conv1D(100, 10, activation='relu'))
model.add(Conv1D(100, 10, activation='relu'))
model.add(MaxPooling1D(3))
model.add(Conv1D(160, 10, activation='relu'))
model.add(Conv1D(160, 10, activation='relu'))
model.add(GlobalAveragePooling1D())
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid')) 
model.summary()
model.compile(loss='binary_crossentropy',
        optimizer='rmsprop',
        metrics=['accuracy'])


X_train = np.expand_dims(X_train, 2)
X_test = np.expand_dims(X_test, 2)


network_history = model.fit(X_train,Y_train, batch_size = batch, epochs = epochs 
                            , validation_split=0.2)
score = model.evaluate(X_test, Y_test, batch_size = batch)
score

y_pred = model.predict(X_test, batch_size = batch)
threshold = 0.5
y_pred1 = y_pred[:,0] < threshold
cm = confusion_matrix(Y_test, y_pred1)
cm

# Plot themresults 
def plot_history(network_history):
  history = network_history.history
  losses = history['loss']
  val_losses = history['val_loss']
  accuracies = history['accuracy']
  val_accuracies = history['val_accuracy']
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.plot(losses)
  plt.plot(val_losses)
  plt.legend(['loss' , 'val_loss'])


  plt.figure()
  plt.xlabel("Epochs")
  plt.ylabel("Accuracy")
  plt.plot(accuracies)
  plt.plot(val_accuracies)
  plt.legend(['laccuracy' , 'val_accuracy'])



plot_history(network_history)